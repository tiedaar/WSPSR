# WSPSR
A brand new way to encode/decode audio

## Overview
### An encoder/decoder audio model
All current audio transformers are encoder-only, meaning that they must be finetuned. This can cause some problems:
* Machine learning is good at cheating
* Finetuned models are prone to overfitting

### Datasets - Supervised, Unsupervised, Weakly Supervised
* Not a lot of supervised data is available. Chan et al. only got 5,140 hours
* Unsupervised data can be easier to find (Zhang et al. got 1,000,000 hours) but is noisier
* Weak supervision uses data that is labeled by machine
* WSPSR uses 680,000 hours of weakly supervised labeled audio data.
  * 117,000 hours are in 96 non-English languages
  * 125,000 hours of x -> en translation data
#### WSPSR's Weakly Supervised Annotation Process
 1. Trained on transcripted audio from the internet
 2. Subpar and machine generated transcripts are automatically detected and removed
 3. Audio language detector was used to annotate language
 4. Performed deduplication and manual inspection

### The Model
* Audio is resampled at 16,000 Hz
* 80-channel log-magnitude Mel spectrogram computed on 25ms windows with stride of 10 ms
* Input is normalized
* Features are extracted with small CNN then fed to encoder

## Questions

## Architecture
![WSPSR pipeline](/pictures/wspsr-pipeline.png)

**Input**: ğ’›, ğ’™ âˆˆ ğ‘‰*, two sequences of token IDs.  
**Output**: ğ‘· âˆˆ (0, 1)	<sup>ğ‘<sub>V</sub>Ã—length(ğ’™)</sup>, where the ğ‘¡-th column of ğ‘· represents ğ‘ƒË†ğœ½(ğ‘¥ [ğ‘¡ + 1] | ğ’™[1 : ğ‘¡], ğ’›).  
**Hyperparameters**: *l*<sub>max</sub>, ğ¿, ğ», ğ‘‘<sub>e</sub>, ğ‘‘<sub>mlp</sub> âˆˆ â„•  
**Parameters**: ğœ½ includes all of the following parameters:  
* ğ‘¾<sub>ğ’†</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘<sub>V</sub></sup> , ğ‘¾<sub>ğ’‘</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—*l*<sub>max</sub></sup> , the token and positional embedding matrices.
* For ğ‘™ âˆˆ [ğ¿]:
 *| W<sub>ğ‘™</sub>, multi-head attention parameters for layer ğ‘™, see (4),
 *| ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>, ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, two sets of layer-norm parameters,


Input: ğ’™ âˆˆ ğ‘‰âˆ—, a sequence of token IDs.Output: ğ‘· âˆˆ (0, 1)	<sup>ğ‘<sub>V</sub>Ã—length(ğ’™)</sup>, where the ğ‘¡-th column of ğ‘· represents ğ‘ƒË†ğœ½(ğ‘¥ [ğ‘¡ + 1] | ğ’™[1 : ğ‘¡]).Hyperparameters: *l*<sub>max</sub>, ğ¿, ğ», ğ‘‘<sub>e</sub>, ğ‘‘<sub>mlp</sub> âˆˆ â„•Parameters: ğœ½ includes all of the following parameters:ğ‘¾<sub>ğ’†</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘<sub>V</sub></sup> , ğ‘¾<sub>ğ’‘</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—*l*<sub>max</sub></sup> , the token and positional embedding matrices.For ğ‘™ âˆˆ [ğ¿]:| W<sub>ğ‘™</sub>, multi-head attention parameters for layer ğ‘™, see (4),| ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>, ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, two sets of layer-norm parameters,| ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub>Ã—ğ‘‘<sub>e</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub></sup>, ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘‘<sub>mlp</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, MLP parameters.ğœ¸, ğœ· âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, final layer-norm parameters.ğ‘¾<sub>ğ’–</sub> âˆˆ â„<sup>ğ‘<sub>V</sub>Ã—ğ‘‘<sub>e</sub></sup>, the unembedding matrix.1 *l* â† length(ğ’™)2 for ğ‘¡ âˆˆ [*l*] : ğ’†<sub>ğ‘¡</sub> â† ğ‘¾<sub>ğ’†</sub> [:, ğ‘¥ [ğ‘¡]] + ğ‘¾<sub>ğ’‘</sub> [:, ğ‘¡]3 ğ‘¿ â† [ğ’†<sub>1</sub>, ğ’†<sub>2</sub>, . . . ğ’†<sub>*l*</sub>]4 for ğ‘™ = 1, 2, . . . , ğ¿ do5 | for ğ‘¡ âˆˆ [*l*] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>)6 | ğ‘¿ â† ğ‘¿ + MHAttention(ğ‘¿Ëœ |W<sub>ğ‘™</sub>, Mask[ğ‘¡, ğ‘¡'] = [[ğ‘¡ â‰¤ ğ‘¡']])7 | for ğ‘¡ âˆˆ [*l*] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>)8 | ğ‘¿ â† ğ‘¿ + ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub>GELU(ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub>ğ‘¿Ëœ + ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub>1<sup>T</sup>) + ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub>1<sup>T</sup>9 end10 for ğ‘¡ âˆˆ [*l*] : ğ‘¿[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸, ğœ·)11 return ğ‘· = softmax(ğ‘¾<sub>ğ’–</sub>ğ‘¿)

## Critical Analysis
### Low Resource Languages
### Low Quality Data
### Additional Tasks

## Links

## Video

## Code Demonstration

## References

<a id="1">[1]</a> 
Radford, A., Kim, J.W., Tao, X., Brockman, G., McLeavey, C., & Sutskever, I. (2022). 
Robust Speech Recognition via Large-Scale Weak Supervision.
Technical report, OpenAI, 2022. URL https://cdn.openai.com/papers/whisper.pdf.
