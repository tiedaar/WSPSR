# WSPSR
A brand new way to encode/decode audio

## Overview
### An encoder/decoder audio model
All current audio transformers are encoder-only, meaning that they must be finetuned. This can cause some problems:
* Machine learning is good at cheating
* Finetuned models are prone to overfitting

### Datasets - Supervised, Unsupervised, Weakly Supervised
* Not a lot of supervised data is available. Chan et al. only got 5,140 hours
* Unsupervised data can be easier to find (Zhang et al. got 1,000,000 hours) but is noisier
* Weak supervision uses data that is labeled by machine
* WSPSR uses 680,000 hours of weakly supervised labeled audio data.
  * 117,000 hours are in 96 non-English languages
  * 125,000 hours of x -> en translation data
#### WSPSR's Weakly Supervised Annotation Process
 1. Trained on transcripted audio from the internet
 2. Subpar and machine generated transcripts are automatically detected and removed
 3. Audio language detector was used to annotate language
 4. Performed deduplication and manual inspection

### The Model
* Audio is resampled at 16,000 Hz
* 80-channel log-magnitude Mel spectrogram computed on 25ms windows with stride of 10 ms
* Input is normalized
* Features are extracted with small CNN then fed to encoder

## Questions

## Architecture
![WSPSR pipeline](/pictures/wspsr-pipeline.png)

Input: ğ’™ âˆˆ ğ‘‰âˆ—, a sequence of token IDs.

Output: ğ‘· âˆˆ (0, 1)	<sup>ğ‘<sub>V</sub>Ã—length(ğ’™)</sup>, where the ğ‘¡-th column of ğ‘· represents ğ‘ƒË†ğœ½(ğ‘¥ [ğ‘¡ + 1] | ğ’™[1 : ğ‘¡]).

Hyperparameters: *l*<sub>max</sub>, ğ¿, ğ», ğ‘‘<sub>e</sub>, ğ‘‘<sub>mlp</sub> âˆˆ â„•

Parameters: ğœ½ includes all of the following parameters:
 ğ‘¾<sub>ğ’†</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘<sub>V</sub></sup> , 
 ğ‘¾<sub>ğ’‘</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—*l*<sub>max</sub></sup> , the token and positional embedding matrices.
 For ğ‘™ âˆˆ [ğ¿]:| W<sub>ğ‘™</sub>, multi-head attention parameters for layer ğ‘™, see (4),| ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>, ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, two sets of layer-norm parameters,| ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub>Ã—ğ‘‘<sub>e</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub></sup>, ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘‘<sub>mlp</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, MLP parameters.ğœ¸, ğœ· âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, final layer-norm parameters.ğ‘¾<sub>ğ’–</sub> âˆˆ â„<sup>ğ‘<sub>V</sub>Ã—ğ‘‘<sub>e</sub></sup>, the unembedding matrix.1 *l* â† length(ğ’™)2 for ğ‘¡ âˆˆ [*l*] : ğ’†<sub>ğ‘¡</sub> â† ğ‘¾<sub>ğ’†</sub> [:, ğ‘¥ [ğ‘¡]] + ğ‘¾<sub>ğ’‘</sub> [:, ğ‘¡]3 ğ‘¿ â† [ğ’†<sub>1</sub>, ğ’†<sub>2</sub>, . . . ğ’†<sub>*l*</sub>]4 for ğ‘™ = 1, 2, . . . , ğ¿ do5 | for ğ‘¡ âˆˆ [*l*] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>)6 | ğ‘¿ â† ğ‘¿ + MHAttention(ğ‘¿Ëœ |W<sub>ğ‘™</sub>, Mask[ğ‘¡, ğ‘¡'] = [[ğ‘¡ â‰¤ ğ‘¡']])7 | for ğ‘¡ âˆˆ [*l*] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>)8 | ğ‘¿ â† ğ‘¿ + ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub>GELU(ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub>ğ‘¿Ëœ + ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub>1<sup>T</sup>) + ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub>1<sup>T</sup>9 end10 for ğ‘¡ âˆˆ [*l*] : ğ‘¿[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸, ğœ·)11 return ğ‘· = softmax(ğ‘¾<sub>ğ’–</sub>ğ‘¿)


Algorithm 8: ğ‘· EDTransformerÂ¹ğ’›Â” ğ’™jğœ½Âº /* Encoder-decoder transformer forward pass */ Input: ğ’›Â” ğ’™ 2 ğ‘‰, two sequences of token IDs. Output: ğ‘· 2 Â¹0Â” 1Âºğ‘VlengthÂ¹ğ’™Âº , where the ğ‘¡-th column of ğ‘· represents Ë† ğ‘ƒğœ½ Â¹ğ‘¥ Â»ğ‘¡  Ì§ 1Â¼ j ğ’™ Â»1 : ğ‘¡Â¼Â” ğ’›Âº. Hyperparameters: maxÂ” ğ¿encÂ” ğ¿decÂ” ğ»Â” ğ‘‘eÂ” ğ‘‘mlp 2 â„• Parameters: ğœ½ includes all of the following parameters: ğ‘¾ğ’† 2 â„ğ‘‘eğ‘V , ğ‘¾ğ’‘ 2 â„ğ‘‘emax , the token and positional embedding matrices. For ğ‘™ 2 Â»ğ¿encÂ¼: j Wenc ğ‘™ , multi-head encoder attention parameters for layer ğ‘™, see (4), j ğœ¸1 ğ‘™ Â” ğœ·1 ğ‘™ Â” ğœ¸2 ğ‘™ Â” ğœ·2 ğ‘™ 2 â„ğ‘‘e, two sets of layer-norm parameters, j ğ‘¾ğ‘™ mlp1 2 â„ğ‘‘mlpğ‘‘e , ğ’ƒğ‘™ mlp1 2 â„ğ‘‘mlp , ğ‘¾ğ‘™ mlp2 2 â„ğ‘‘eğ‘‘mlp , ğ’ƒğ‘™ mlp2 2 â„ğ‘‘e, MLP parameters. For ğ‘™ 2 Â»ğ¿decÂ¼: j Wdec ğ‘™ , multi-head decoder attention parameters for layer ğ‘™, see (4), j WeÂd ğ‘™ , multi-head cross-attention parameters for layer ğ‘™, see (4), j ğœ¸3 ğ‘™ Â” ğœ·3 ğ‘™ Â” ğœ¸4 ğ‘™ Â” ğœ·4 ğ‘™ Â” ğœ¸5 ğ‘™ Â” ğœ·5 ğ‘™ 2 â„ğ‘‘e, three sets of layer-norm parameters, j ğ‘¾ğ‘™ mlp3 2 â„ğ‘‘mlpğ‘‘e , ğ’ƒğ‘™ mlp3 2 â„ğ‘‘mlp , ğ‘¾ğ‘™ mlp4 2 â„ğ‘‘eğ‘‘mlp , ğ’ƒğ‘™ mlp4 2 â„ğ‘‘e, MLP parameters. ğ‘¾ğ’– 2 â„ğ‘Vğ‘‘e, the unembedding matrix. /* Encode the context sequence: */ 1 z lengthÂ¹ğ’›Âº 2 for ğ‘¡ 2 Â»zÂ¼ : ğ’†ğ‘¡ ğ‘¾ğ’† Â»:Â” ğ‘§Â»ğ‘¡Â¼Â¼  Ì§ ğ‘¾ğ’‘ Â»:Â” ğ‘¡Â¼ 3 ğ’ Â»ğ’†1Â” ğ’†2Â” Â“ Â“ Â“ ğ’†z Â¼ 4 for ğ‘™ = 1Â” 2Â” Â“ Â“ Â“ Â” ğ¿enc do 5 ğ’ ğ’  Ì§ MHAttentionÂ¹ğ’ jWenc ğ‘™ Â” Mask  1Âº 6 for ğ‘¡ 2 Â»zÂ¼ : ğ’Â»:Â” ğ‘¡Â¼ layer_normÂ¹ğ’Â»:Â” ğ‘¡Â¼ j ğœ¸1 ğ‘™ Â” ğœ·1 ğ‘™Âº 7 ğ’ ğ’  Ì§ ğ‘¾ğ‘™ mlp2 ReLU Â¹ğ‘¾ ğ‘™ mlp1ğ’  Ì§ ğ’ƒğ‘™ mlp11áµ€Âº  Ì§ ğ’ƒğ‘™ mlp21áµ€ 8 for ğ‘¡ 2 Â»zÂ¼ : ğ’Â»:Â” ğ‘¡Â¼ layer_normÂ¹ğ’Â»:Â” ğ‘¡Â¼ j ğœ¸2 ğ‘™ Â” ğœ·2 ğ‘™Âº 9 end /* Decode the primary sequence, conditioning on the context: */ 10 x lengthÂ¹ğ’™Âº 11 for ğ‘¡ 2 Â»xÂ¼ : ğ’†ğ‘¡ ğ‘¾ğ’† Â»:Â” ğ‘¥ Â»ğ‘¡Â¼Â¼  Ì§ ğ‘¾ğ’‘ Â»:Â” ğ‘¡Â¼ 12 ğ‘¿ Â»ğ’†1Â” ğ’†2Â” Â“ Â“ Â“ ğ’†x Â¼ 13 for ğ‘– = 1Â” 2Â” Â“ Â“ Â“ Â” ğ¿dec do 14 ğ‘¿ ğ‘¿  Ì§ MHAttentionÂ¹ğ‘¿ jWdec ğ‘™ Â” MaskÂ»ğ‘¡Â” ğ‘¡0Â¼  Â»Â»ğ‘¡  ğ‘¡0Â¼Â¼Âº 15 for ğ‘¡ 2 Â»xÂ¼ : ğ‘¿ Â»:Â” ğ‘¡Â¼ layer_normÂ¹ğ‘¿ Â»:Â” ğ‘¡Â¼ j ğœ¸3 ğ‘™ Â” ğœ·3 ğ‘™Âº 16 ğ‘¿ ğ‘¿  Ì§ MHAttentionÂ¹ğ‘¿Â” ğ’ jWeÂd ğ‘™ Â” Mask  1Âº 17 for ğ‘¡ 2 Â»xÂ¼ : ğ‘¿ Â»:Â” ğ‘¡Â¼ layer_normÂ¹ğ‘¿ Â»:Â” ğ‘¡Â¼ j ğœ¸4 ğ‘™ Â” ğœ·4 ğ‘™Âº 18 ğ‘¿ ğ‘¿  Ì§ ğ‘¾ğ‘™ mlp4 ReLU Â¹ğ‘¾ ğ‘™ mlp3 ğ‘¿  Ì§ ğ’ƒğ‘™ mlp31áµ€Âº  Ì§ ğ’ƒğ‘™ mlp41áµ€ 19 for ğ‘¡ 2 Â»xÂ¼ : ğ‘¿ Â»:Â” ğ‘¡Â¼ layer_normÂ¹ğ‘¿ Â»:Â” ğ‘¡Â¼ j ğœ¸5 ğ‘™ Â” ğœ·5 ğ‘™Âº 20 end /* Derive conditional probabilities and return: */ 21 return ğ‘· = softmaxÂ¹ğ‘¾ğ’–ğ‘¿Âº

## Critical Analysis
### Low Resource Languages
### Low Quality Data
### Additional Tasks

## Links

## Video

## Code Demonstration

## References

<a id="1">[1]</a> 
Radford, A., Kim, J.W., Tao, X., Brockman, G., McLeavey, C., & Sutskever, I. (2022). 
Robust Speech Recognition via Large-Scale Weak Supervision.
Technical report, OpenAI, 2022. URL https://cdn.openai.com/papers/whisper.pdf.
