# WSPSR
A brand new way to encode/decode audio

## Overview
### An encoder/decoder audio model
All current audio transformers are encoder-only, meaning that they must be finetuned. This can cause some problems:
* Machine learning is good at cheating
* Finetuned models are prone to overfitting

### Datasets - Supervised, Unsupervised, Weakly Supervised
* Not a lot of supervised data is available. Chan et al. only got 5,140 hours
* Unsupervised data can be easier to find (Zhang et al. got 1,000,000 hours) but is noisier
* Weak supervision uses data that is labeled by machine
* WSPSR uses 680,000 hours of weakly supervised labeled audio data.
  * 117,000 hours are in 96 non-English languages
  * 125,000 hours of x -> en translation data
  * fdsaf
#### WSPSR's Weakly Supervised Annotation Process
 1. Trained on transcripted audio from the internet
 2. Subpar and machine generated transcripts are automatically detected and removed
 3. Audio language detector was used to annotate language
 4. Performed deduplication and manual inspection

### The Model
* Audio is resampled at 16,000 Hz
* 80-channel log-magnitude Mel spectrogram computed on 25ms windows with stride of 10 ms
* Input is normalized
* Features are extracted with small CNN then fed to encoder

## Questions

## Architecture
![WSPSR pipeline](/pictures/wspsr-pipeline.png)

**Input**: ğ’› âˆˆ ğ‘‰*<sub>ğ’›</sub>, sequence of Mel spectrogram coefficients; ğ’™ âˆˆ ğ‘‰*<sub>ğ’™</sub>, a sequence of token IDs.  
**Output**: ğ‘· âˆˆ (0, 1)	<sup>ğ‘<sub>V</sub>Ã—length(ğ’™)</sup>, where the ğ‘¡-th column of ğ‘· represents ğ‘ƒË†ğœ½(ğ‘¥ [ğ‘¡ + 1] | ğ’™[1 : ğ‘¡], ğ’›).  
**Hyperparameters**: *l*<sub>max</sub>, ğ¿, ğ», ğ‘‘<sub>e</sub>, ğ‘‘<sub>mlp</sub> âˆˆ â„•  
**Parameters**: ğœ½ includes all of the following parameters:  
* ğ‘¾<sub>ğ’†</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘<sub>V</sub></sup> , ğ‘¾<sub>ğ’‘</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—*l*<sub>max</sub></sup> , the token and positional embedding matrices.
* For ğ‘™ âˆˆ [ğ¿<sub>enc</sub>]:
  * | W<sub>ğ‘™</sub>, multi-head attention parameters for layer ğ‘™, see (4),
  * | ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>, ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, two sets of layer-norm parameters,
  * | ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub>Ã—ğ‘‘<sub>e</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub></sup>, ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘‘<sub>mlp</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, MLP parameters.  
* For ğ‘™ âˆˆ [ğ¿<sub>dec</sub>]:
  * | W<sub>ğ‘™</sub>, multi-head attention parameters for layer ğ‘™, see (4),    
  * | W<sup>e/d</sup>, multi-head cross-attention parameters for layer ğ‘™, see (4),
  * | ğœ¸<sup>3</sup><sub>ğ‘™</sub>, ğœ·<sup>3</sup><sub>ğ‘™</sub>, ğœ¸<sup>4</sup><sub>ğ‘™</sub>, ğœ·<sup>4</sup><sub>ğ‘™</sub>, ğœ¸<sup>5</sup><sub>ğ‘™</sub>, ğœ·<sup>5</sup><sub>ğ‘™</sub>âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, three sets of layer-norm parameters,
  * | ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub>Ã—ğ‘‘<sub>e</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub> âˆˆ â„<sup>ğ‘‘<sub>mlp</sub></sup>, ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub>Ã—ğ‘‘<sub>mlp</sub></sup>, ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub> âˆˆ â„<sup>ğ‘‘<sub>e</sub></sup>, MLP parameters.
 * ğ‘¾<sub>ğ’–</sub> âˆˆ â„<sup>ğ‘<sub>V</sub>Ã—ğ‘‘<sub>e</sub></sup>, the unembedding matrix.  

_encode the context sequence_
1. *l*<sub>z</sub> â† length(ğ’›)
2. for ğ‘¡ âˆˆ [*l*<sub>z</sub>] : ğ’†<sub>ğ‘¡</sub> â† 2 x conv(ğ’›[ğ‘¡], GELU) + ğ‘¾<sub>ğ’‘</sub> [:, ğ‘¡]
3. ğ‘¿ â† [ğ’†<sub>1</sub>, ğ’†<sub>2</sub>, . . . ğ’†<sub>*l*</sub>]
4. for ğ‘™ = 1, 2, . . . , ğ¿ do
5. * | ğ’ â† ğ’ + MHAttention(ğ’| ğ‘¾<sup>enc</sup><sub>l</sub> ğ‘™, Mask = 1)
6. * | for ğ‘¡ âˆˆ [*l,<sub>z</sub>*] : ğ’†<sub>ğ‘¡</sub> â† layer_norm(ğ’[:,ğ‘¡]|ğœ¸<sup>1</sup><sub>ğ‘™</sub>, ğœ·<sup>1</sup><sub>ğ‘™</sub>)
7. * | ğ’ â† ğ’ + ğ‘¾<sup>ğ‘™</sup><sub>mlp2</sub>ReLU(ğ‘¾<sup>ğ‘™</sup><sub>mlp1</sub>ğ’+ğ’ƒ<sup>ğ‘™</sup><sub>mlp1</sub>) + ğ’ƒ<sup>ğ‘™</sup><sub>mlp2</sub>**1**<sup>T</sup>
8. * | for ğ‘¡ âˆˆ [*l,<sub>z</sub>*]: ğ’[:,t] â† layer_norm(ğ’[:,t]|ğœ¸<sup>2</sup><sub>ğ‘™</sub>, ğœ·<sup>2</sup><sub>ğ‘™</sub>)
9. **end**  
_decode the primary sequence, conditioning on the context_
11.  *l*<sub>x</sub> â† length(ğ’™)
12.  for ğ‘¡ âˆˆ [*l*<sub>x</sub>] : ğ’†<sub>ğ‘¡</sub> â† ğ‘¾<sub>ğ’†</sub> [:, ğ‘¥ [ğ‘¡]] + ğ‘¾<sub>ğ’‘</sub> [:, ğ‘¡]
13.  ğ‘¿ â† [ğ’†<sub>1</sub>, ğ’†<sub>2</sub>, . . . ğ’†<sub>*l*</sub>]
14.  for i<sub>dec</sub> = 1, 2, . . . , ğ¿ **do**
15.  * | ğ‘¿ â† ğ‘¿ + MHAttention(ğ‘¿ |W<sub>ğ‘™</sub><sup>dec</sup>, Mask[ğ‘¡, ğ‘¡'] = [[ğ‘¡ â‰¤ ğ‘¡']])
16.  * | for ğ‘¡ âˆˆ [*l*<sub>x</sub>] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>3</sup><sub>ğ‘™</sub>, ğœ·<sup>3</sup><sub>ğ‘™</sub>)
17.  * | ğ‘¿ â† ğ‘¿ + MHAttention(ğ‘¿ |W<sub>ğ‘™</sub><sup>e/d</sup>, Mask = 1)
18.  * | for ğ‘¡ âˆˆ [*l*<sub>x</sub>] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>4</sup><sub>ğ‘™</sub>, ğœ·<sup>4</sup><sub>ğ‘™</sub>)
19.  * | ğ‘¿ â† ğ‘¿ + ğ‘¾<sup>ğ‘™</sup><sub>mlp4</sub>ReLU(ğ‘¾<sup>ğ‘™</sup><sub>mlp3</sub>ğ‘¿+ğ’ƒ<sup>ğ‘™</sup><sub>mlp3</sub>) + ğ’ƒ<sup>ğ‘™</sup><sub>mlp4</sub>**1**<sup>T</sup>
20.  * | for ğ‘¡ âˆˆ [*l*<sub>x</sub>] : ğ‘¿Ëœ[:, ğ‘¡] â† layer_norm(ğ‘¿[:, ğ‘¡] | ğœ¸<sup>5</sup><sub>ğ‘™</sub>, ğœ·<sup>5</sup><sub>ğ‘™</sub>)
21.  **end**  
_derive conditional probabilities and return_
21.  **return _P_** = softmax(ğ‘¾<sub>u</sub>ğ‘¿)


## Critical Analysis
### Low Resource Languages
### Low Quality Data
### Additional Tasks

## Links

## Video

## Code Demonstration

## References

<a id="1">[1]</a> 
Radford, A., Kim, J.W., Tao, X., Brockman, G., McLeavey, C., & Sutskever, I. (2022). 
Robust Speech Recognition via Large-Scale Weak Supervision.
Technical report, OpenAI, 2022. URL https://cdn.openai.com/papers/whisper.pdf.
